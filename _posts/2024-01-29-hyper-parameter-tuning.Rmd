---
layout: post
title: The importance of hyper-parameter tuning
description: Comparing with no tuning
---

```{r Ropts, echo=FALSE}
repo.dir <- normalizePath("..")
post.id <- "2024-01-29-hyper-parameter-tuning"
fig.path <- file.path(repo.dir, "assets", "img", post.id)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=paste0(fig.path, "/"),
  fig.width=8,
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

The goal of this blog post is to compare a machine learning
algorithms, and demonstrate the importance of learning model
complexity hyper-parameters, if the goal is to minimize prediction
error (and that is always the goal in ML).

## Theoretical discussion and visualizations

What is model complexity and why does it need to be controlled? Model
complexity is the ability of a learning algorithm to fit complex
patterns which are encoded in the train set. There is a different
measure of model complexity for every learning algorithm:

* number of neighbors in K-nearest-neighbors,
* degree of L1 regularization in LASSO (as implemented in glmnet package in R),
* degree of polynomial in linear model basis expansion,
* cost parameter in support vector machines,
* number of iterations and learning rate in boosting (as implemented
  in xgboost package in R),
* number of gradient descent iterations (epochs), learning rate,
  number of hidden units, number of layers, etc, in neural networks.

In each machine learning algorithm, the model complexity must be
controlled, by selecting a good value for the corresponding
hyper-parameter. I created [a data
visualization](https://tdhock.github.io/2023-12-04-degree-neighbors/)
which shows how this works using nearest neighbors (need to learn
number of neighbors) and linear models (need to learn the degree of the
polynomial basis expansion), for several regression problems.

* There are four different simulated data sets, each with a pattern of
  different complexity (constant, linear, quadratic, cubic).
* The linear model polynomial degree varies from 0 (constant
  prediction function, most regularized, least complex) to 9 (very
  wiggly prediction function that perfectly interpolates every train
  data point, least regularized, most complex).
* The number of neighbors varies from 1 (very wiggly prediction
  function that perfectly interpolates every train data point, least
  regularized, most complex) to 10 (constant prediction function, most
  regularized, least complex).
* These two learning algorithms are interesting examples to compare,
  because the least complex model (in both cases) always predicts the
  average of the train labels. That turns out to be the best
  prediction function, in the case of the constant data/pattern.
* But for the other data/patterns, a more complex regularization
  parameter must be used to get optimal prediction accuracy. For
  example, the regularization parameters which gives minimal test
  error for the linear data/pattern are 3 nearest neighbors, and
  polynomial degree 1.

The goal of this blog is to demonstrate how to use [mlr3 auto
tuner](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#sec-autotuner)
to properly learn such hyper-parameters, and show that approach is
more accurate than using the defaults (which may be ok for some data,
but sub-optimal for others).

## Simulation

Below we define some simulated data, for five different regression
problems, of varying complexity.

```{r simData}
max.x <- 12
min.x <- -max.x
fun.list <- list(
  constant=function(x)1,
  linear=function(x)x/3,
  quadratic=function(x)x^2/max.x-5,
  sin=function(x)4*sin(x),
  step=function(x)ifelse(x<0, 4, -4))
N <- 200
set.seed(1)
input.vec <- runif(N, min.x, max.x)
library(data.table)
task.list <- list()
sim.dt.list <- list()
for(fun.name in names(fun.list)){
  f <- fun.list[[fun.name]]
  true.vec <- f(input.vec)
  task.dt <- data.table(
    input=input.vec,
    output=true.vec+rnorm(N,sd=2))
  task.list[[fun.name]] <- mlr3::TaskRegr$new(
    fun.name, task.dt, target="output"
  )
  sim.dt.list[[fun.name]] <- data.table(fun.name, task.dt)
}
(sim.dt <- rbindlist(sim.dt.list))
library(ggplot2)
ggplot()+
  geom_point(aes(
    input, output),
    data=sim.dt)+
  facet_grid(. ~ fun.name, labeller=label_both)
```

The plot above visualizes the simulated data. The goal of each
learning algorithm is to learn the pattern in each of the five
different data sets. To do that, we define a few different learning
algorithms in the next section.

## Learning algorithm definitions

Perhaps the simplest learning algorithm is nearest neighbors, which
can be implemented in the mlr3 framework using the code below.

```{r}
nn.default <- mlr3learners::LearnerRegrKKNN$new()
nn.default$param_set
```

The code above defines the default nearest neighbors regressor, which
has a default value of `k=7` (that is the number of neighbors).  As
discussed above, the number of neighbors is an important model
complexity hyper-parameter, which must be tuned for optimal prediction
accuracy. In other words, the default value 7 may provide sub-optimal
prediction accuracy, depending on the data set. Another option is to
manually select a different value for the number of neighbors, which
we do in the code below,

```{r}
nn1 <- mlr3learners::LearnerRegrKKNN$new()
nn1$id <- "1 nearest neighbor"
nn1$param_set$values$k <- 1
```

The code above defines another nearest neighbor regressor, which uses
1 nearest neighbor, instead of the default 7. That may be better for
some data sets, but in general, we would like to choose a different
number of neighbors for each data set. We can do that using
cross-validation, by dividing the train set into subtrain/validation
sets, where the validation set is used to determine the optimal number
of neighbors. The code below defines 5-fold cross-validation as the
method to use,

```{r}
subtrain.valid.cv <- mlr3::ResamplingCV$new()
subtrain.valid.cv$param_set$values$folds <- 5
```

Next, in the code below, we define a new nearest neighbor regressor,
and tell mlr3 to tune the number of neighbors.

```{r}
knn.learner <- mlr3learners::LearnerRegrKKNN$new()
knn.learner$param_set$values$k <- paradox::to_tune(1, 20)
```

To convert that to a learning algorithm that can be used in a
benchmark experiment, we need to use as the `learner` in an
`auto_tuner`, as in the code below,

```{r}
knn.tuned = mlr3tuning::auto_tuner(
  tuner = mlr3tuning::TunerGridSearch$new(),
  learner = knn.learner,
  resampling = subtrain.valid.cv,
  measure = mlr3::msr("regr.mse"))
```

The code above says that we want to tune the learner using grid
search, 5-fold CV, and select the model which minimizes MSE on the
validation set.

Another non-linear regression algorithm is boosting, which can be
implemented in mlr3 using the code below. Again there are several
hyper-parameters which control model complexity, and in the code below
we tune two of them (eta=step size, nrounds=number of boosting
iterations). Note that some parameters should be tuned on the log
scale (like eta with `log=TRUE` in the code below), and others can be
tuned on the linear scale (like nrounds). Finally note in the code
below that we use a grid search with resolution 5 in order to save
time (default is 10 so with 2 hyper-parameters that would make 100
combinations, exercise for the reader to try that instead).

```{r}
xgboost.learner <- mlr3learners::LearnerRegrXgboost$new()
xgboost.learner$param_set$values$eta <- paradox::to_tune(0.001, 1, log=TRUE)
xgboost.learner$param_set$values$nrounds <- paradox::to_tune(1, 100)
grid.search.5 <- mlr3tuning::TunerGridSearch$new()
grid.search.5$param_set$values$resolution <- 5
xgboost.tuned = mlr3tuning::auto_tuner(
  tuner = grid.search.5,
  learner = xgboost.learner,
  resampling = subtrain.valid.cv,
  measure = mlr3::msr("regr.mse"))
```

You might wonder about the code above, how did I know to tune eta from
0.001 to 1, and nrounds from 1 to 100? What are the good min/max
values for each hyper-parameter?
[mlr3tuningspaces](https://mlr-org.com/tuning_spaces.html) is a useful
reference with data describing typical values for each
hyper-parameter.  And in fact, if you want to use the hyper-parameter
ranges defined in that package, you can use code like below (ranger
package for random forest).

```{r}
ranger.tuned = mlr3tuning::auto_tuner(
  tuner = grid.search.5,
  learner = mlr3tuningspaces::lts(mlr3learners::LearnerRegrRanger$new()),
  resampling = subtrain.valid.cv,
  measure = mlr3::msr("regr.mse"))
```

## Define benchmark experiment

After having defined the learning algorithms in the previous section,
we combine them in the list below.

```{r}
learner.list <- list(
  mlr3::LearnerRegrFeatureless$new(),
  mlr3learners::LearnerRegrRanger$new(), ranger.tuned,
  mlr3learners::LearnerRegrXgboost$new(), xgboost.tuned,
  knn.tuned, nn1, nn.default)
```

To see which of the learning algorithms is most accurate in each data
set, we will use 3-fold cross-validation, as defined in ht code below.

```{r}
train.test.cv <- mlr3::ResamplingCV$new()
train.test.cv$param_set$values$folds <- 3
```

In the code below, we define a benchmark grid, which combines tasks
(data sets), with learners (algorithms), and resamplings (actually
just one, 3-fold CV).

```{r}
(bench.grid <- mlr3::benchmark_grid( 
  tasks=task.list,
  learners=learner.list,
  resamplings=train.test.cv))
```

In the code below we tell the mlr3 logger to suppress most messages,
then we run the benchmark experiment. Actually, if the results have
already been computed and saved in the cache RData file, we just read
that. Otherwise, you should run the code below that
interactively, either on your own computer, or on a SLURM super-computer cluster.
There are only 120 iterations in this case, but if you run that on a
super-computer cluster, then it could potentially run 120x faster.
It is better to use a
super-computer for larger experiments, with more data sets, learning
algorithms, and cross-validation folds. For example I have recently
computed some machine learning experiments with over 1000 benchmark
iterations, each of which can be computed in parallel (and very
quickly using NAU Monsoon which has 4000 CPUs). See
[mlr3batchmark](https://mlr3batchmark.mlr-org.com/) for computing
these iterations in parallel using the batchtools R package (which
supports backends such as SLURM, which is how things are parallelized
on super-computers such as NAU Monsoon). See my [R batchtools on
Monsoon](https://tdhock.github.io/blog/2020/monsoon-batchtools/)
tutorial for more info.

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
cache.RData <- "2024-01-29-hyper-parameter-tuning.RData"
if(file.exists(cache.RData)){
  load(cache.RData)
}else{#code below should be run interactively.
  if(on.cluster){
    reg.dir <- "2024-01-29-hyper-parameter-tuning-registry"
    unlink(reg.dir, recursive=TRUE)
    reg = batchtools::makeExperimentRegistry(
      file.dir = reg.dir,
      seed = 1,
      packages = "mlr3verse"
    )
    mlr3batchmark::batchmark(
      bench.grid, store_models = TRUE, reg=reg)
    job.table <- batchtools::getJobTable(reg=reg)
    chunks <- data.frame(job.table, chunk=1)
    batchtools::submitJobs(chunks, resources=list(
      walltime = 60*60,#seconds
      memory = 2000,#megabytes per cpu
      ncpus=1,  #>1 for multicore/parallel jobs.
      ntasks=1, #>1 for MPI jobs.
      chunks.as.arrayjobs=TRUE), reg=reg)
    batchtools::getStatus(reg=reg)
    jobs.after <- batchtools::getJobTable(reg=reg)
    table(jobs.after$error)
    ids <- jobs.after[is.na(error), job.id]
    bench.result <- mlr3batchmark::reduceResultsBatchmark(ids, reg = reg)
  }else{
    ## In the code below, we declare a multisession future plan to
    ## compute each benchmark iteration in parallel on this computer
    ## (data set, learning algorithm, cross-validation fold). For a
    ## few dozen iterations, using the multisession backend is
    ## probably sufficient (I have 12 CPUs on my work PC).
    if(require(future))plan("multisession")
    bench.result <- mlr3::benchmark(
      bench.grid, store_models = TRUE)
  }    
  save(bench.result, file=cache.RData)
}
```

If you are following along, now is the time to grab a cup of tea or coffee.

## Comparing tuned to default/fixed hyper-parameters

After having computed the benchmark result in the previous section, we
use the score method below to compute a data table with columns for
train time and mean squared test error.

```{r}
bench.score <- bench.result$score(mlr3::msrs(c("time_train","regr.mse")))
bench.score[1]
```

First, we create a new algorithm factor column in the code below, and
take the subset of results for the nearest neighbors learning algorithms. 

```{r}
algo.levs <- c(
  "regr.ranger.tuned", "regr.ranger", 
  "regr.xgboost.tuned", "regr.xgboost", 
  "regr.kknn.tuned", nn.default$id, nn1$id,
  "regr.featureless")
only.nn <- bench.score[
, algorithm := factor(learner_id, algo.levs)
][
  grepl("kknn", algorithm)
]
```

We use the code below to visualize the nearest neighbors test error
values.

```{r nnError}
ggplot()+
  geom_point(aes(
    regr.mse, algorithm),
    data=only.nn)+
  facet_grid(. ~ task_id, labeller=label_both, scales="free")+
  scale_x_log10(
    "Mean squared prediction error on the test set")
```

The plot above shows the mean squared prediction error on the X axis, and the learning algorithm on the Y axis. TODO

The code below computes the selected number of neighbors for the tuned
nearest neighbor algorithms:

```{r}
only.nn[
  grepl("tuned", algorithm),
  sapply(learner, function(L)L$tuning_result$k)
]
```

It is clear that the tuning procedure selected numbers of neighbors
which are different from the fixed values we used in the other
learners (1 and 7).

Next, we examine the test error rates of the random forest learners.

```{r rangerError}
only.ranger <- bench.score[grepl("ranger", algorithm)]
ggplot()+
  geom_point(aes(
    regr.mse, algorithm),
    data=only.ranger)+
  facet_grid(. ~ task_id, labeller=label_both, scales="free")+
  scale_x_log10(
    "Mean squared prediction error on the test set")
```

It is clear from the plot above that the tuned version is at least as
accurate as the default, for each data set. TODO

Next, we examine the test error rates of the xgboost learners.

```{r xgboostError}
only.xgboost <- bench.score[grepl("xgboost", algorithm)]
ggplot()+
  geom_point(aes(
    regr.mse, algorithm),
    data=only.xgboost)+
  facet_grid(. ~ task_id, labeller=label_both, scales="free")+
  scale_x_log10(
    "Mean squared prediction error on the test set")
```

The plot above shows that the tuned learner is at least as accurate as
the default, for each data set.

## Comparing learning algorithms

In this section we compare the tuned versions of the learning
algorithms with each other.

```{r tunedError}
only.tuned <- bench.score[grepl("tuned", algorithm)]
ggplot()+
  geom_point(aes(
    regr.mse, algorithm),
    data=only.tuned)+
  facet_grid(. ~ task_id, labeller=label_both, scales="free")+
  scale_x_log10(
    "Mean squared prediction error on the test set")
```

It is clear that the most accurate algorithm is TODO.

## Comparing train time

In this section we compare the time it takes to train the algorithms.

```{r}
ggplot()+
  geom_point(aes(
    time_train, algorithm),
    data=bench.score)+
  facet_grid(. ~ task_id, labeller=label_both, scales="free")+
  scale_x_log10(
    "Training time (seconds)")
```
  
## Conclusions

TODO

## Session info

```{r}
sessionInfo()
```

