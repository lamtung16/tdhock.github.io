---
layout: post
title: Plotting the probability simplex
description: An application of matrix inversion 
output:
---

```{r Ropts, echo=FALSE}
repo.dir <- normalizePath("..")
post.id <- "2022-02-15-simplex-"
fig.path <- file.path(repo.dir, "assets", "img", post.id)
knitr::opts_chunk$set(
  dpi=200,
  fig.path=fig.path,
  fig.width=10,
  fig.height=6)
Sys.setenv(RETICULATE_PYTHON=if(.Platform$OS.type=="unix")
  "/home/tdhock/.local/share/r-miniconda/envs/cs570s22/bin/python"
  else "~/Miniconda3/envs/cs570s22/python.exe")
reticulate::use_condaenv("cs570s22", required=TRUE)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
if(FALSE){
  knitr::knit("2022-02-15-simplex.Rmd")
}
rendering <- in_render || in_knit
```

```{python echo=FALSE}
repo_dir = r["repo.dir"]
fig_path = r["fig.path"]
import warnings
def p9_save(g, name):
    out_png = fig_path+name+".png"
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        g.save(out_png)
    web_link = out_png.replace(repo_dir, "")
    print('![plot of %s](%s)'%(name, web_link))
# work-around for rendering plots under windows, which hangs within
# emacs python shell: instead write a PNG file and view in browser.
import os
import webbrowser
on_windows = os.name == "nt"
rendering = r.rendering if 'r' in dir() else False
using_agg = on_windows and not rendering
if using_agg:
    import matplotlib
    matplotlib.use("agg")
def show(g, name):
    if not using_agg:
        return p9_save(g, name)
    g.save("tmp.png")
    webbrowser.open('tmp.png')
```

For my [CS570 class this semester on Deep
Learning](https://github.com/tdhock/cs570-spring-2022), I prepared
some figures which compare the logistic loss to the zero-one loss in
binary classification. I also wanted to show the loss functions for
multi-class classification. This post explains how I did that in
python using numpy, pandas, and plotnine. First I define the loss
functions in terms of the real-valued prediction scores `f` and the
label `y` (either -1 or 1),

```{python}
import numpy as np
loss_dict = {
    "logistic":lambda f, y: np.log(1+np.exp(-y*f)),
    "zero-one":lambda f, y: np.where(f>0, 1, -1)!=y,
}
```

Then we compute those loss functions for both labels, on a grid of
predicted scores from -5 to 5,

```{python}
import pandas as pd
pred_lim = 5
pred_grid = np.linspace(-pred_lim, pred_lim)
loss_df_list = []
for loss_name, loss_fun in loss_dict.items():
    for y in -1, 1:
        loss_df_list.append(pd.DataFrame({
            "loss_name":loss_name,
            "loss_value":loss_fun(pred_grid, y),
            "predicted_score":pred_grid,
            "label":y,
        }))
loss_df = pd.concat(loss_df_list)
loss_df
```

Then we plot these loss values with one panel for each label,

```{python results='asis'}
import plotnine as p9
def gg_binary(x):
    return p9.ggplot()+\
        p9.facet_grid(". ~ label", labeller="label_both")+\
        p9.scale_x_continuous(breaks=np.arange(-5, 7, 0.5))+\
        p9.theme_bw()+\
        p9.theme(subplots_adjust={'right': 0.7, "bottom":0.2})+\
        p9.theme(figure_size=(4.5,2))+\
        p9.geom_point(
            p9.aes(
                x=x,
                y="loss_value",
                color="loss_name",
                ),
            data=loss_df)
show(gg_binary("predicted_score"), "binary-loss-scores")
```

Next we can plot the loss as a function of predicted probability of
class 1,

```{python results='asis'}
loss_df["pred_prob1"] = 1/(1+np.exp(-loss_df.predicted_score))
show(gg_binary("pred_prob1"), "binary-loss-prob")
```

How to generalize this plot to more classes than two? We can plot the
loss functions for three classes on the probability simplex,
projected onto the cartesian plotting plane as an equilateral
triangle. First we compute the vertices of the equilateral triangle,

```{python}
xmax = 1.0
upper_x = xmax/2.0
ymax = np.sqrt(xmax - upper_x**2)
vertices_mat = np.array([
    [xmax, 0, 1],
    [upper_x, ymax, 1],
    [0,0,1]
])
vertices_mat
```

The first two columns of the matrix above represent the vertices of
the equilateral triangle. Each point inside this triangle represents a
triple of probability values on the simplex, which sum to one and are
all at least zero. To plot the loss function for each probability
triple in the simplex, we can make a heat map by first computing a
grid of (x,y) values:

```{python}
def make_grid(mat, n_grid = 200):
    nrow, ncol = mat.shape
    assert ncol == 2
    mesh_args = mat.apply(
        lambda x: np.linspace(min(x),max(x), n_grid), axis=0)
    mesh_tup = np.meshgrid(*[mesh_args[x] for x in mesh_args])
    mesh_vectors = [v.flatten() for v in mesh_tup]
    return pd.DataFrame(dict(zip(mesh_args,mesh_vectors)))
simplex_grid = make_grid(pd.DataFrame({
    "x":np.linspace(0,xmax),
    "y":np.linspace(0,ymax)
}))
simplex_grid
```

Now which of these rows above falls within the equilateral triangle,
and is therefore a valid probability triple in the simplex? We need a
mapping from these (x,y) coordinates to the probability triple
coordinates, which we can get by solving a linear system:
vertices_mat * A = I.  The matrix A is the linear transformation which
converts x,y coordinates to probability triple coordinates (identity
matrix I represents unit vectors which are the vertices of the simplex
in probability coordinates). To solve for A we just need to matrix
multiply both sides by the inverse,

```{python}
to_prob_mat = np.linalg.inv(vertices_mat)
to_prob_mat
```

We then can convert the x,y coordinates of the grid to probability
coordinates:

```{python}
simplex_grid_xy = np.column_stack(
    [simplex_grid, np.repeat(1, simplex_grid.shape[0])])
simplex_grid_prob = np.matmul(simplex_grid_xy, to_prob_mat)
simplex_grid_prob
```

Then we exclude the rows with any negative probability values,

```{python}
keep = simplex_grid_prob.min(axis=1) >= 0
keep_grid = pd.concat([
    pd.DataFrame(simplex_grid_prob), simplex_grid
], axis=1)[keep]
keep_grid
```

Next we compute the logistic loss for each one of those grid points,
and for each of the three labels,

```{python}
def get_loss_df(loss_fun):
    loss_simplex_list = []
    for label in range(3):
        loss_only = pd.DataFrame({
            "loss":loss_fun(label),
            "label":label,
        })
        loss_grid = pd.concat([keep_grid.reset_index(), loss_only], axis=1)
        loss_simplex_list.append(loss_grid)
    return pd.concat(loss_simplex_list)
def logistic_loss(label, loss_max = 5):
    label_prob = keep_grid[label]
    loss_vec = np.log(1/label_prob)
    # threshold loss for visualization purposes, to avoid saturation.
    return np.where(loss_vec<loss_max, loss_vec, loss_max)
logistic_df = get_loss_df(logistic_loss)
logistic_df
```

Finally, we plot the loss values as a heatmap on the equilateral
triangle which represents the probability simplex,

```{python results='asis'}
def gg_loss(loss_name, grid_loss_df, breaks=[0,0.5,1]):
    return p9.ggplot()+\
        p9.ggtitle(loss_name+" loss on 3-simplex")+\
        p9.facet_grid(". ~ label", labeller="label_both")+\
        p9.geom_tile(
            p9.aes(
                x="x",
                y="y",
                fill="loss",
                ),
            data=grid_loss_df)+\
        p9.scale_fill_gradient(
            low="white",
            high="red")+\
        p9.scale_x_continuous(
            breaks=breaks)+\
        p9.scale_y_continuous(
            breaks=breaks)+\
        p9.coord_equal()
show(gg_loss("Logistic", logistic_df), "multi-logistic")
```

Next, we can compute the zero-one loss in a similar fashion,

```{python}
def zero_one_loss(label):
    loss_vec = np.array(keep_grid.loc[:,[0,1,2]]).argmax(axis=1) != label
    return np.where(loss_vec, 1, 0)
zero_one_df = get_loss_df(zero_one_loss)
zero_one_df
```

```{python results='asis'}
show(gg_loss("Zero-one", zero_one_df), "multi-zero-one")
```

References:
- https://en.wikipedia.org/wiki/Simplex#The_standard_simplex
