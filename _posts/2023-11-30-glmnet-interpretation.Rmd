---
layout: post
title: Interpretation of learning algorithms
description: Regularized linear model and decision tree
---

```{r Ropts, echo=FALSE}
repo.dir <- normalizePath("..")
post.id <- "2023-11-30-glmnet-interpretation"
fig.path <- file.path(repo.dir, "assets", "img", post.id)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=paste0(fig.path,"/"),
  fig.width=8,
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

Machine learning algorithms input a train data set, and output a
prediction function. This post is about interpreting that prediction
function, in terms of what input features in the data are used to
compute predictions. 

### Introduction to model interpretation

Most machine learning algorithms output a prediction function that
uses all of the input features in the train data set. In the special
case of feature selection algorithms, a subset of input features is
used in the prediction function. For example, the L1 regularized
linear learning algorithm (R package glmnet) outputs a
coefficient/weight vector with some values set to zero. We can
therefore say that the model is interpretable in terms of the
different input feature subsets:

* For the features with weights equal to zero, these features are
  completely ignored for the purposes of prediction (non-important
  subset of features).
* For the features with weights not equal to zero, these features are
  used to compute predictions (important subset of features).

In the next sections, we explain how to compute and interpret this
algorithm using base R.

### Data simulation

For the purposes of simulation, we use the simulated data below:

```{r}
N <- 3000
library(data.table)
(full.dt <- data.table(
  label=factor(rep(c("spam","not spam"), l=N))
))
signal <- ifelse(full.dt$label=="not spam", 0, 1)
```

We can imagine a spam filtering system, with training data for which each row in the table above represents a message which has been labeled as spam or not.
To do that we will need some features, which we generate/simulate below:

```{r}
set.seed(1)
n.noise <- 20
full.dt[
, x0 := signal+rnorm(N)
][
, paste0("x",1:n.noise) := replicate(n.noise, rnorm(N), simplify=FALSE)
][]
```

In the table above, there are two sets of features:

* `x0` is the only feature which is correlated with the output `label` (should be the only feature used in the best prediction function)
* `x1` through `x20` are noise features which are random (should be ignored by the best prediction function)
  
In the next section, we run the L1 regularized linear learning
algorithm on these data.

### mlr3 training

```{r}
(task.classif <- mlr3::TaskClassif$new(
  "simulated", full.dt, target="label"
)$set_col_roles("label", c("target", "stratum")))
size_cv <- mlr3resampling::ResamplingVariableSizeTrainCV$new()
size_cv$param_set$values$min_train_data <- 20
cv_glmnet <- mlr3learners::LearnerClassifCVGlmnet$new()
cv_glmnet$param_set$values$nfolds <- 5
(learner.list <- list(
  cv_glmnet,
  mlr3::LearnerClassifRpart$new(),
  mlr3::LearnerClassifFeatureless$new()))
(bench.grid <- mlr3::benchmark_grid(
  task.classif,
  learner.list,
  size_cv))
if(require(future))plan("multisession")
lgr::get_logger("mlr3")$set_threshold("warn")
(bench.result <- mlr3::benchmark(
  bench.grid, store_models = TRUE))

bench.score <- mlr3resampling::score(bench.result)
bench.score[1]

train_size_vec <- unique(bench.score$train_size)
if(require(animint2)){
  ggplot()+
    scale_x_log10()+
    scale_y_log10(
      "Classification error on test set")+
    geom_line(aes(
      train_size, classif.ce,
      group=paste(algorithm, seed),
      color=algorithm),
      shape=1,
      data=bench.score)+
    geom_point(aes(
      train_size, classif.ce, color=algorithm),
      shape=1,
      data=bench.score)+
    facet_grid(
      test.fold~task_id,
      labeller=label_both,
      scales="free")
}

library(glmnet)
glmnet.score <- bench.score[algorithm=="cv_glmnet"]
weight.dt.list <- list()
levs <- names(full.dt)[-1]
for(score.i in 1:nrow(glmnet.score)){
  score.row <- glmnet.score[score.i]
  fit <- score.row$learner[[1]]$model
  weight.mat <- coef(fit)[-1,]
  weight.dt.list[[score.i]] <- score.row[, .(
    test.fold, seed, train_size,
    weight=as.numeric(weight.mat),
    variable=factor(names(weight.mat), levs))]
}
(weight.dt <- rbindlist(weight.dt.list))

ggplot()+
  facet_grid(test.fold ~ train_size, labeller=label_both)+
  scale_y_discrete(breaks=levs,drop=FALSE)+
  geom_tile(aes(
    seed, variable, fill=weight),
    data=weight.dt[weight!=0])+
  scale_fill_gradient2()
    
```

TODO for each variable, count folds with non-zero weight -- 10 folds
instead of 3?

TODO rpart interpretation.
