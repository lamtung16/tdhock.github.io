---
layout: post
title: Interpretation of learning algorithms
description: Regularized linear model
---

```{r Ropts, echo=FALSE}
repo.dir <- normalizePath("..")
post.id <- "2023-11-30-glmnet-interpretation"
fig.path <- file.path(repo.dir, "assets", "img", post.id)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=paste0(fig.path,"/"),
  fig.width=8,
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

Machine learning algorithms input a train data set, and output a
prediction function. This post is about interpreting that prediction
function, in terms of what input features in the data are used to
compute predictions. 

### Introduction to model interpretation

Most machine learning algorithms output a prediction function that
uses all of the input features in the train data set. In the special
case of feature selection algorithms, a subset of input features is
used in the prediction function. For example, the L1 regularized
linear learning algorithm (R package glmnet) outputs a
coefficient/weight vector with some values set to zero. We can
therefore say that the model is interpretable in terms of the
different input feature subsets:

* For the features with weights equal to zero, these features are
  completely ignored for the purposes of prediction (non-important
  subset of features).
* For the features with weights not equal to zero, these features are
  used to compute predictions (important subset of features).

In the next sections, we explain how to compute and interpret this
algorithm using base R.

### Data simulation

For the purposes of simulation, we use the simulated data below:

```{r}
N <- 200
library(data.table)
(full.dt <- data.table(
  label=factor(rep(c("spam","not spam"), l=N)),
  person=rep(1:2, each=0.5*N)
)[, signal := ifelse(label=="not spam", 0, 3)][])
```

Above each row has an person ID between 1 and 2. 
We can imagine a spam filtering system, that has training data for multiple people (here just two).
Each row in the table above represents a message which has been labeled as spam or not, by one of the two people.
Can we train on one person, and accurately predict on the other person? And can we interpret what input features are important for each person?
To do that we will need some features, which we generate/simulate below:

```{r}
set.seed(1)
n.people <- length(unique(full.dt$person))
for(person.i in 1:n.people){
  use.signal.vec <- list(
    easy=rep(if(person.i==1)TRUE else FALSE, N),
    impossible=full.dt$person==person.i)
  for(task_id in names(use.signal.vec)){
    use.signal <- use.signal.vec[[task_id]]
    full.dt[
    , paste0("x",person.i,"_",task_id) := ifelse(
      use.signal, signal, 0
    )+rnorm(N)][]
  }
}
full.dt
```

In the table above, there are two sets of two features:

* For easy features, one is correlated with the label (`x1_easy`), and
  one is random noise (`x2_easy`), so the algorithm just needs to
  learn to ignore the noise feature, and concentrate on the signal
  feature. That should be possible given data from either person (same
  signal in each person).
* Each impossible feature is correlated with the label (when feature
  number same as person number), or is just noise (when person number
  different from feature number). So if the algorithm has access to
  the correct person (same as test, say person 2), then it needs to
  learn to use the corresponding feature `x2_impossible`. But if
  the algorithm does not have access to that person, then the best it
  can do is same as featureless (predict most frequent class label in
  train data).
  
### Static visualization of simulated data

Below we reshape the data to a table which is more suitable for visualization:

```{r}
(scatter.dt <- nc::capture_melt_multiple(
  full.dt,
  column="x[12]",
  "_",
  task_id="easy|impossible"))
```

Below we visualize the pattern for each person and feature type:

```{r}
if(require(animint2)){
  ggplot()+
    geom_point(aes(
      x1, x2, color=label),
      shape=1,
      data=scatter.dt)+
    facet_grid(
      task_id ~ person,
      labeller=label_both)
}
```

In the plot above, it is apparent that 

* for easy features (left), the two label classes differ in x1 values
  for both people (and x2 is not important for prediction). So it
  should be possible/easy to train on person 1, and predict accurately
  on person 2, by only using x1.
* for impossible features (right), the two people have different label
  patterns. For person 1, the two label classes differ in x1 values
  (x2 is not important), whereas for person 2, the two label classes
  differ in x2 values (x1 is not important). So it should be
  impossible to train on person 1, and predict accurately on
  person 2. And when interpreting the learned linear model
  coefficients/weights, we should see these different features as
  being important for the two different people.
  
In the next section, we run the L1 regularized linear learning
algorithm on these data.

### mlr3 training

TODO interpretation of glmnet models, after running a mlr3 benchmark.
