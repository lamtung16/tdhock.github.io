---
layout: post
title: Learning with Area Under the Min
description: How to use torch with a non-standard loss 
---

```{r Ropts, echo=FALSE}
repo.dir <- normalizePath("..")
post.id <- "2022-04-19-aum-learning-"
fig.path <- file.path(repo.dir, "assets", "img", post.id)
knitr::opts_chunk$set(
  dpi=200,
  fig.path=fig.path,
  fig.width=10,
  fig.height=6)
Sys.setenv(RETICULATE_PYTHON=if(.Platform$OS.type=="unix")
  "/home/tdhock/.local/share/r-miniconda/envs/cs570s22/bin/python"
  else "~/Miniconda3/envs/cs570s22/python.exe")
reticulate::use_condaenv("cs570s22", required=TRUE)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
if(FALSE){
  knitr::knit("2022-04-19-aum-learning.Rmd")
}
rendering <- in_render || in_knit
```

```{python echo=FALSE}
repo_dir = r["repo.dir"]
fig_path = r["fig.path"]
import warnings
def p9_save(g, name):
    out_png = fig_path+name+".png"
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        g.save(out_png)
    web_link = out_png.replace(repo_dir, "")
    print('![plot of %s](%s)'%(name, web_link))
# work-around for rendering plots under windows, which hangs within
# emacs python shell: instead write a PNG file and view in browser.
import os
import webbrowser
on_windows = os.name == "nt"
rendering = r.rendering if 'r' in dir() else False
using_agg = on_windows and not rendering
if using_agg:
    import matplotlib
    matplotlib.use("agg")
def show(g, name):
    if not using_agg:
        return p9_save(g, name)
    g.save("tmp.png")
    webbrowser.open('tmp.png')
```

My collaborator Joe Barr of [Acronis](https://acronisscs.com/) is
working on an application of machine learning to predicting security
vulnerabilities based on source code analysis, and a database of
exploits. The data they are using for training/testing their models is
highly imbalanced so I suggested trying my new [Area Under the
Min(FP,FN) loss function (AUM)](https://arxiv.org/abs/2107.01285).
After talking with them about the easiest way to try this new loss
function on their data, [I coded it in
torch](https://tdhock.github.io/blog/2022/torch-auto-grad-non-diff/)
and verified that the gradients computed by torch are reasonable with
respect to the expected directional directives. But does it work for
learning? Yes, it does, as I show below.

## Reading and scaling spam data

As an example data set, consider the spam data, one of the [examples
from the Elements of Statistical Learning
textbook](https://hastie.su.domains/ElemStatLearn/data.html).

```{python}
import pandas as pd
spam_df = pd.read_csv(
    "~/teaching/cs570-spring-2022/data/spam.data",
    header=None,
    sep=" ")
spam_df
```

After reading them into python above, we scale and center the
inputs/features below,

```{python}
import numpy as np
spam_features = spam_df.iloc[:,:-1].to_numpy()
spam_labels = spam_df.iloc[:,-1].to_numpy()
# 1. feature scaling
spam_mean = spam_features.mean(axis=0)
spam_sd = np.sqrt(spam_features.var(axis=0))
scaled_features = (spam_features-spam_mean)/spam_sd
scaled_features.mean(axis=0)
scaled_features.var(axis=0)
scaled_features
```

The above output indicates that each feature column is indeed scaled
to mean 0 and variance 1. 

## Dividing the data into subtrain and validation sets

Next, we randomly divide the data into a
subtrain and validation set. The gradient descent parameter updates
will be computed using the subtrain set, and the validation set will
be used to check for overfitting.

```{python}
np.random.seed(1)
n_folds = 5
fold_vec = np.random.randint(low=0, high=n_folds, size=spam_labels.size)
validation_fold = 0
is_set_dict = {
    "validation":fold_vec == validation_fold,
    "subtrain":fold_vec != validation_fold,
}
```

After having defined logical vectors above that indicate each set, we
use them below to construct the corresponding tensors,

```{python}
import torch
set_features = {}
set_labels = {}
for set_name, is_set in is_set_dict.items():
    set_features[set_name] = torch.from_numpy(
        scaled_features[is_set,:]).float()
    set_labels[set_name] = torch.from_numpy(
        spam_labels[is_set]).float()
{set_name:array.shape for set_name, array in set_features.items()}
```

The output above shows the number of rows and columns in each data
set.

## Defining Dataset and DataLoader, neural network, loss function

To do the gradient descent learning randomly with a given batch size,
it is easiest in torch to create a Dataset sub-class and a DataLoader
instance, as we do below:

```{python}
class CSV(torch.utils.data.Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels
    def __getitem__(self, item):
        return self.features[item,:], self.labels[item]
    def __len__(self):
        return len(self.labels)
subtrain_dataset = CSV(set_features["subtrain"], set_labels["subtrain"])
subtrain_dataloader = torch.utils.data.DataLoader(
    subtrain_dataset, batch_size=20, shuffle=True)
```

Next we define a neural network below with a single hidden layer of
200 hidden units,

```{python}
nrow, ncol = set_features["subtrain"].shape
class NNet(torch.nn.Module):
    def __init__(self, n_hidden=200):
        super(NNet, self).__init__()
        self.seq = torch.nn.Sequential(
            torch.nn.Linear(ncol, n_hidden),
            torch.nn.ReLU(),
            torch.nn.Linear(n_hidden, 1)
            )
    def forward(self, feature_mat):
        return self.seq(feature_mat)
```

Below we define the AUM loss function and a helper function which we
use to compute the loss during each gradient descent batch, and at the
end of every epoch during the subtrain/validation loss computation,

```{python}
def AUM(pred_tensor, label_tensor):
    """Area Under Min(FP,FN)

    Loss function for imbalanced binary classification
    problems. Minimizing AUM empirically results in maximizing Area
    Under the ROC Curve (AUC). Arguments: pred_tensor and label_tensor
    should both be 1d tensors (vectors of real-valued predictions and
    labels for each observation in the set/batch).

    """
    fn_diff = torch.where(label_tensor == 1, -1, 0)
    fp_diff = torch.where(label_tensor == 1, 0, 1)
    thresh_tensor = -pred_tensor.flatten()
    sorted_indices = torch.argsort(thresh_tensor)
    sorted_fp_cum = fp_diff[sorted_indices].cumsum(axis=0)
    sorted_fn_cum = -fn_diff[sorted_indices].flip(0).cumsum(axis=0).flip(0)
    sorted_thresh = thresh_tensor[sorted_indices]
    sorted_is_diff = sorted_thresh.diff() != 0
    sorted_fp_end = torch.cat([sorted_is_diff, torch.tensor([True])])
    sorted_fn_end = torch.cat([torch.tensor([True]), sorted_is_diff])
    uniq_thresh = sorted_thresh[sorted_fp_end]
    uniq_fp_after = sorted_fp_cum[sorted_fp_end]
    uniq_fn_before = sorted_fn_cum[sorted_fn_end]
    uniq_min = torch.minimum(uniq_fn_before[1:], uniq_fp_after[:-1])
    return torch.mean(uniq_min * uniq_thresh.diff())

def compute_loss_pred(features, labels):
    pred_mat = model(features)
    pred_vec = pred_mat.reshape(len(pred_mat))
    return AUM(pred_vec, labels), pred_vec

from pytorch_lightning.metrics.classification import AUROC
compute_auc = AUROC()
```

We additionally use
[AUROC](https://pytorch-lightning.readthedocs.io/en/0.8.5/metrics.html#auroc)
to compute the Area Under the ROC Curve.

## Learning iterations

Finally we use a for loop over epochs below to do the gradient descent
learning. Note the step size of the Adam optimizer was chosen to be
small enough such that the loss decreases smoothly (no abrupt jumps
up), but big enough so that the loss is minimized within a reasonable
number of epochs.

```{python}
loss_df_list = []
max_epochs=100
torch.manual_seed(1)
model = NNet()
optimizer = torch.optim.Adam(model.parameters(), lr=0.00004)
for epoch in range(max_epochs):
    # first update weights.
    for batch_features, batch_labels in subtrain_dataloader:
        optimizer.zero_grad()
        batch_loss, pred_vec = compute_loss_pred(batch_features, batch_labels)
        batch_loss.backward()
        optimizer.step()
    # then compute subtrain/validation loss.
    for set_name in set_features:
        feature_mat = set_features[set_name]
        label_vec = set_labels[set_name]
        set_loss, pred_vec = compute_loss_pred(feature_mat, label_vec)
        loss_df_list.append(pd.DataFrame({
            "epoch":[epoch],
            "set_name":[set_name],
            "variable":["AUM"],
            "value":set_loss.detach().numpy(),
            }))
        loss_df_list.append(pd.DataFrame({
            "epoch":[epoch],
            "set_name":[set_name],
            "variable":["AUC"],
            "value":compute_auc(pred_vec, label_vec).detach().numpy(),
            }))
loss_df = pd.concat(loss_df_list)
```

Finally we visualize the subtrain/validation loss and AUC using plotnine,

```{python}
import plotnine as p9
gg = p9.ggplot()+\
    p9.facet_grid("variable ~ .", labeller="label_both", scales="free")+\
    p9.geom_line(
        p9.aes(
            x="epoch",
            y="value",
            color="set_name"
        ),
        data=loss_df)
show(gg, "AUM-AUC")
```

The plot above shows that the algorithm clearly is learning, and that
decreasing the AUM results in increased AUC. 
